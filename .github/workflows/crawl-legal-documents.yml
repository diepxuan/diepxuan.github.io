name: Crawl Vietnamese Legal Documents

on:
  schedule:
    # Cháº¡y hÃ ng tuáº§n vÃ o thá»© 2 lÃºc 2:00 AM UTC (9:00 AM GMT+7)
    - cron: '0 2 * * 1'
  workflow_dispatch:  # Cho phÃ©p cháº¡y thá»§ cÃ´ng
  push:
    branches:
      - main
    paths:
      - '.github/workflows/crawl-legal-documents.yml'
      - 'scripts/crawl-legal-documents.py'

jobs:
  crawl-legal-documents:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 lxml pandas markdownify python-dateutil || echo "Dependencies installed with possible warnings"
        
    - name: Create scripts directory
      run: mkdir -p scripts
    
    - name: Create crawler script
      run: |
        cat > scripts/crawl-legal-documents.py << 'EOF'
#!/usr/bin/env python3
"""
Crawler for Vietnamese Legal Documents
Crawls legal documents from various Vietnamese government sources
"""

import os
import json
import requests
import pandas as pd
from datetime import datetime
from bs4 import BeautifulSoup
from markdownify import markdownify
from dateutil import parser
import time
import re

class VietnameseLegalCrawler:
    def __init__(self):
        self.base_urls = {
            'vanban_chinhphu': 'https://vanban.chinhphu.vn',
            'thuvienphapluat': 'https://thuvienphapluat.vn',
            'moj': 'https://moj.gov.vn'
        }
        
        self.output_dir = 'van-ban/crawled'
        os.makedirs(self.output_dir, exist_ok=True)
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
        })
    
    def crawl_vanban_chinhphu(self):
        """Crawl documents from vanban.chinhphu.vn"""
        print("Crawling vanban.chinhphu.vn...")
        
        try:
            # URL for latest documents
            url = f"{self.base_urls['vanban_chinhphu']}/portal/page/portal/chinhphu/hethongvanban"
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            documents = []
            
            # Find document links (adjust selector based on actual site structure)
            doc_links = soup.select('a[href*="vanban"]')
            
            for link in doc_links[:10]:  # Limit to 10 documents for testing
                doc_url = link.get('href')
                if doc_url and not doc_url.startswith('http'):
                    doc_url = f"{self.base_urls['vanban_chinhphu']}{doc_url}"
                
                doc_title = link.get_text(strip=True)
                
                if doc_title and doc_url:
                    documents.append({
                        'source': 'vanban_chinhphu',
                        'title': doc_title,
                        'url': doc_url,
                        'crawled_at': datetime.now().isoformat()
                    })
            
            return documents
            
        except Exception as e:
            print(f"Error crawling vanban.chinhphu.vn: {e}")
            return []
    
    def crawl_thuvienphapluat(self):
        """Crawl documents from thuvienphapluat.vn"""
        print("Crawling thuvienphapluat.vn...")
        
        try:
            # Search for latest documents
            search_url = f"{self.base_urls['thuvienphapluat']}/tim-van-ban"
            params = {
                'keyword': '',
                'type': '0',
                'status': '0',
                'sort': '1'  # Sort by latest
            }
            
            response = self.session.get(search_url, params=params, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            documents = []
            
            # Find document items (adjust selector)
            doc_items = soup.select('.document-item, .vb-item')
            
            for item in doc_items[:10]:  # Limit to 10
                title_elem = item.select_one('.title, .vb-title')
                link_elem = item.select_one('a')
                
                if title_elem and link_elem:
                    doc_title = title_elem.get_text(strip=True)
                    doc_url = link_elem.get('href')
                    
                    if doc_url and not doc_url.startswith('http'):
                        doc_url = f"{self.base_urls['thuvienphapluat']}{doc_url}"
                    
                    # Try to extract document number and date
                    doc_number = self.extract_document_number(doc_title)
                    doc_date = self.extract_document_date(item)
                    
                    documents.append({
                        'source': 'thuvienphapluat',
                        'title': doc_title,
                        'url': doc_url,
                        'document_number': doc_number,
                        'issue_date': doc_date,
                        'crawled_at': datetime.now().isoformat()
                    })
            
            return documents
            
        except Exception as e:
            print(f"Error crawling thuvienphapluat.vn: {e}")
            return []
    
    def extract_document_number(self, title):
        """Extract document number from title"""
        # Pattern for Vietnamese document numbers: Sá»‘ 123/2024/NÄ-CP
        patterns = [
            r'Sá»‘\s+(\d+/\d+/\S+)',
            r'Sá»‘:\s+(\d+/\d+/\S+)',
            r'(\d+/\d+/\S+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title)
            if match:
                return match.group(1)
        
        return None
    
    def extract_document_date(self, element):
        """Extract document date from element"""
        # Look for date elements
        date_selectors = ['.date', '.vb-date', '.ngay-ban-hanh', 'time']
        
        for selector in date_selectors:
            date_elem = element.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                try:
                    # Try to parse date
                    parsed_date = parser.parse(date_text, fuzzy=True)
                    return parsed_date.isoformat()
                except:
                    pass
        
        return None
    
    def save_documents(self, documents):
        """Save crawled documents to files"""
        if not documents:
            print("No documents to save")
            return
        
        # Save as JSON
        json_file = os.path.join(self.output_dir, 'legal_documents.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)
        
        # Save as CSV
        csv_file = os.path.join(self.output_dir, 'legal_documents.csv')
        df = pd.DataFrame(documents)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        # Create markdown summary
        self.create_markdown_summary(documents)
        
        print(f"Saved {len(documents)} documents to {self.output_dir}")
    
    def create_markdown_summary(self, documents):
        """Create markdown summary of crawled documents"""
        md_file = os.path.join(self.output_dir, 'README.md')
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# TÃ i liá»‡u PhÃ¡p luáº­t Viá»‡t Nam (Tá»± Ä‘á»™ng thu tháº­p)\n\n")
            f.write(f"*Cáº­p nháº­t láº§n cuá»‘i: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n")
            
            f.write("## ðŸ“Š Thá»‘ng kÃª\n")
            f.write(f"- **Tá»•ng sá»‘ vÄƒn báº£n**: {len(documents)}\n")
            
            # Group by source
            sources = {}
            for doc in documents:
                source = doc['source']
                sources[source] = sources.get(source, 0) + 1
            
            f.write("- **PhÃ¢n bá»• theo nguá»“n**:\n")
            for source, count in sources.items():
                f.write(f"  - {source}: {count} vÄƒn báº£n\n")
            
            f.write("\n## ðŸ“œ Danh sÃ¡ch VÄƒn báº£n\n\n")
            
            # Group documents by source
            grouped_docs = {}
            for doc in documents:
                source = doc['source']
                if source not in grouped_docs:
                    grouped_docs[source] = []
                grouped_docs[source].append(doc)
            
            for source, docs in grouped_docs.items():
                f.write(f"### Nguá»“n: {source}\n\n")
                
                for doc in docs:
                    f.write(f"#### {doc['title']}\n")
                    
                    if doc.get('document_number'):
                        f.write(f"- **Sá»‘ hiá»‡u**: {doc['document_number']}\n")
                    
                    if doc.get('issue_date'):
                        issue_date = doc['issue_date'][:10]  # Just date part
                        f.write(f"- **NgÃ y ban hÃ nh**: {issue_date}\n")
                    
                    f.write(f"- **URL**: [{doc['url']}]({doc['url']})\n")
                    f.write(f"- **Thu tháº­p lÃºc**: {doc['crawled_at'][:19]}\n\n")
            
            f.write("\n## ðŸ”§ Cáº¥u hÃ¬nh\n\n")
            f.write("TÃ i liá»‡u Ä‘Æ°á»£c tá»± Ä‘á»™ng thu tháº­p bá»Ÿi GitHub Actions:\n")
            f.write("- **Lá»‹ch trÃ¬nh**: HÃ ng tuáº§n (thá»© 2, 9:00 AM GMT+7)\n")
            f.write("- **Nguá»“n dá»¯ liá»‡u**:\n")
            f.write("  - vanban.chinhphu.vn\n")
            f.write("  - thuvienphapluat.vn\n")
            f.write("  - moj.gov.vn\n")
            
            f.write("\n## ðŸ“ Ghi chÃº\n\n")
            f.write("1. Dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p tá»± Ä‘á»™ng, cÃ³ thá»ƒ khÃ´ng Ä‘áº§y Ä‘á»§\n")
            f.write("2. Kiá»ƒm tra link gá»‘c Ä‘á»ƒ xem ná»™i dung Ä‘áº§y Ä‘á»§\n")
            f.write("3. BÃ¡o cÃ¡o váº¥n Ä‘á» táº¡i repository GitHub\n")
    
    def run(self):
        """Main execution method"""
        print("Starting Vietnamese Legal Documents Crawler...")
        print(f"Output directory: {self.output_dir}")
        
        all_documents = []
        
        # Crawl from multiple sources
        all_documents.extend(self.crawl_vanban_chinhphu())
        time.sleep(2)  # Be polite
        
        all_documents.extend(self.crawl_thuvienphapluat())
        time.sleep(2)
        
        # Remove duplicates based on URL
        unique_documents = []
        seen_urls = set()
        
        for doc in all_documents:
            if doc['url'] not in seen_urls:
                seen_urls.add(doc['url'])
                unique_documents.append(doc)
        
        print(f"Found {len(unique_documents)} unique documents")
        
        # Save documents
        self.save_documents(unique_documents)
        
        print("Crawling completed successfully!")

if __name__ == "__main__":
    crawler = VietnameseLegalCrawler()
    crawler.run()
EOF
        
        chmod +x scripts/crawl-legal-documents.py
    
    - name: Run crawler
      run: python scripts/crawl-legal-documents.py
      env:
        # Add any required environment variables here
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Check for changes
      id: git-check
      run: |
        git config --global user.email "actions@github.com"
        git config --global user.name "GitHub Actions"
        
        if git diff --quiet; then
          echo "No changes detected"
          echo "has_changes=false" >> $GITHUB_OUTPUT
        else
          echo "Changes detected"
          echo "has_changes=true" >> $GITHUB_OUTPUT
        fi
    
    - name: Commit and push changes
      if: steps.git-check.outputs.has_changes == 'true'
      run: |
        git add documents/van-ban-phap-luat/crawled/
        git add scripts/crawl-legal-documents.py
        git commit -m "docs: Update crawled legal documents [skip ci]"
        git push origin HEAD:feat/legal-documents-crawler
    
    - name: Create Pull Request
      if: steps.git-check.outputs.has_changes == 'true'
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        branch: feat/legal-documents-crawler
        base: main
        title: "docs: Update crawled Vietnamese legal documents"
        body: |
          ## Tá»± Ä‘á»™ng cáº­p nháº­t TÃ i liá»‡u PhÃ¡p luáº­t Viá»‡t Nam
          
          GitHub Action Ä‘Ã£ tá»± Ä‘á»™ng thu tháº­p cÃ¡c vÄƒn báº£n phÃ¡p luáº­t má»›i nháº¥t tá»«:
          - vanban.chinhphu.vn
          - thuvienphapluat.vn
          
          ### Thay Ä‘á»•i:
          - Cáº­p nháº­t file JSON/CSV trong `documents/van-ban-phap-luat/crawled/`
          - Cáº­p nháº­t markdown summary
          
          ### Lá»‹ch trÃ¬nh:
          - Tá»± Ä‘á»™ng cháº¡y hÃ ng tuáº§n (thá»© 2, 9:00 AM GMT+7)
          - CÃ³ thá»ƒ cháº¡y thá»§ cÃ´ng tá»« GitHub Actions
          
          ### Files thay Ä‘á»•i:
          ```bash
          git diff --name-only
          ```
        labels: automated, documentation, legal
        draft: false
    
    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: crawled-legal-documents
        path: |
          van-ban/crawled/
          scripts/crawl-legal-documents.py
        retention-days: 7